{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.memory = deque(maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNConv(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions, n_hidden=256):\n",
    "    super(DQNConv, self).__init__()\n",
    "    self.n_observations = n_observations\n",
    "    self.n_actions = n_actions\n",
    "\n",
    "    # first convolutional layer 4x4x4 => 4x4x8\n",
    "    self.conv1 = nn.Conv2d(4, 8, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn1 = nn.BatchNorm2d(8)\n",
    "    \n",
    "    # second convolutional layer 4x4x8 => 4x4x16\n",
    "    self.conv2 = nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn2 = nn.BatchNorm2d(16)\n",
    "    \n",
    "    # third convolutional layer 4x4x16 => 4x4x32\n",
    "    self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "    # first fully connected layer 512 => 512\n",
    "    self.fc1 = nn.Linear(4*4*32, 4*4*32)\n",
    "    \n",
    "    # second fully connected layer 512 => 256\n",
    "    self.fc2 = nn.Linear(32*4*4, 16*4*4)\n",
    "\n",
    "    # third fully connected layer 256 => 128\n",
    "    self.fc3 = nn.Linear(16*4*4, 8*4*4)\n",
    "\n",
    "    # output layer 128 => 16\n",
    "    self.output_layer = nn.Linear(128, n_actions)\n",
    "\n",
    "    nn.init.xavier_uniform_(self.fc1.weight)\n",
    "    nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    nn.init.xavier_uniform_(self.fc3.weight)\n",
    "    nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 4, 4, 4)\n",
    "\n",
    "    # conv1 + bn1 with activation function ReLU\n",
    "    x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "    \n",
    "    # conv2 + bn2 with activation function ReLU\n",
    "    x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "    \n",
    "    # conv3 + bn3 with activation function ReLU\n",
    "    x = nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "    \n",
    "    # flatten will transform data structure from 3D 8x8x128 to 1D 8192\n",
    "    x = nn.Flatten()(x)\n",
    "    \n",
    "    # fully connected with activation function ReLU\n",
    "    x = nn.functional.relu(self.fc1(x))\n",
    "\n",
    "    # fully connected with activation function ReLU\n",
    "    x = nn.functional.relu(self.fc2(x))\n",
    "\n",
    "    # fully connected with activation function ReLU\n",
    "    x = nn.functional.relu(self.fc3(x))\n",
    "\n",
    "    x = self.output_layer(x)\n",
    "\n",
    "    # x = F.relu(self.fc1(x))\n",
    "    # x = F.relu(self.fc2(x))\n",
    "    # x = F.relu(self.fc3(x))\n",
    "    # x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentConv:\n",
    "  def __init__(self, logging=False, training=False):\n",
    "    self.training = training\n",
    "\n",
    "    self.n_observations = 4*4*4\n",
    "    self.n_actions = 16\n",
    "\n",
    "    self.policy_net = DQNConv(self.n_observations, self.n_actions).to(device)\n",
    "    self.target_net = DQNConv(self.n_observations, self.n_actions).to(device)\n",
    "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    if training:\n",
    "      self.BATCH_SIZE = 256\n",
    "      self.memory = ReplayMemory(100000)\n",
    "      self.steps_done = 0\n",
    "\n",
    "      self.GAMMA = 0.995\n",
    "      self.EPS_START = 0.9\n",
    "      self.EPS_END = 0.05\n",
    "      self.EPS_DECAY = 6000\n",
    "\n",
    "      self.LR = 1e-3\n",
    "      self.TAU = 0.005\n",
    "\n",
    "      self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "      self.loss = nn.MSELoss()\n",
    "    else:\n",
    "      self.policy_net.eval()\n",
    "      self.target_net.eval()\n",
    "\n",
    "  def load_weights(self, path):\n",
    "    state_dict = torch.load(path)\n",
    "    self.policy_net.load_state_dict(state_dict)\n",
    "    self.target_net.load_state_dict(state_dict)\n",
    "    print(\"Weights loaded successfully from\", path)\n",
    "  \n",
    "  def save_weights(self, path):\n",
    "    torch.save(self.policy_net.state_dict(), path)\n",
    "    print(\"Weights saved successfully to\", path)\n",
    "  \n",
    "  def create_indicator_array(self, coords, num_rows=4, num_cols=4):\n",
    "    indicator_array = torch.zeros(16, device=device)\n",
    "    \n",
    "    for row, col in coords:\n",
    "      index = row * num_cols + col\n",
    "      indicator_array[index] = 1\n",
    "    \n",
    "    return indicator_array\n",
    "  \n",
    "  def creat_index_array(self, coords, num_rows=4, num_cols=4):\n",
    "    index_array = []\n",
    "    \n",
    "    for row, col in coords:\n",
    "      index = row * num_cols + col\n",
    "      index_array.append(index)\n",
    "    \n",
    "    return torch.tensor(index_array, device=device)\n",
    "\n",
    "  def findBestMove(self, board, possible_move, player):\n",
    "    if len(possible_move) == 0:\n",
    "      return None\n",
    "    \n",
    "    if self.training:\n",
    "      sample = random.random()\n",
    "      eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "      self.steps_done += 1\n",
    "\n",
    "      if sample > eps_threshold:\n",
    "        self.policy_net.eval()\n",
    "        self.target_net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "          # Change the board to the player's perspective\n",
    "          state = torch.tensor(board, dtype=torch.float32).to(device)\n",
    "          mask = self.create_indicator_array(possible_move)\n",
    "          score = self.policy_net(state) * mask - 9999 * (1-mask)\n",
    "          return score.max(1).indices.view(1, 1)\n",
    "      else:\n",
    "        return torch.tensor([[random.choice(self.creat_index_array(possible_move))]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "        # Change the board to the player's perspective\n",
    "        state = torch.tensor(board, dtype=torch.float32).to(device)\n",
    "        mask = self.create_indicator_array(possible_move)\n",
    "        score = self.policy_net(state) * mask - 9999 * (1-mask)\n",
    "        return score.max(1).indices.view(1, 1)\n",
    "  \n",
    "  def optimize_model(self):\n",
    "    self.policy_net.train()\n",
    "    self.target_net.train()\n",
    "\n",
    "    if len(self.memory) < self.BATCH_SIZE:\n",
    "      return\n",
    "    transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "    loss = self.loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "    # for param in self.policy_net.parameters():\n",
    "    #   param.grad.data.clamp_(-1, 1)\n",
    "    self.optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
