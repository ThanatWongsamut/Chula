{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TicTacToe3D\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'environment'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from environment import TicTacToe3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.memory = deque(maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions, n_hidden=128):\n",
    "    super(DQN, self).__init__()\n",
    "    self.n_observations = n_observations\n",
    "    self.n_actions = n_actions\n",
    "\n",
    "    self.layer1 = nn.Linear(n_observations, n_hidden)\n",
    "    self.layer2 = nn.Linear(n_hidden, n_hidden)\n",
    "    self.layer3 = nn.Linear(n_hidden, n_actions)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.relu(self.layer1(x))\n",
    "    x = F.relu(self.layer2(x))\n",
    "    x = self.layer3(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "  def __init__(self, logging=False, training=False, weight_path=None):\n",
    "    self.training = training\n",
    "\n",
    "    self.n_observations = 4*4*4\n",
    "    self.n_actions = 16\n",
    "\n",
    "    self.policy_net = DQN(self.n_observations, self.n_actions).to(device)\n",
    "    self.target_net = DQN(self.n_observations, self.n_actions).to(device)\n",
    "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    if training:\n",
    "      self.BATCH_SIZE = 128\n",
    "      self.memory = ReplayMemory(10000)\n",
    "      self.steps_done = 0\n",
    "\n",
    "      self.GAMMA = 0.99\n",
    "      self.EPS_START = 0.9\n",
    "      self.EPS_END = 0.05\n",
    "      self.EPS_DECAY = 1000\n",
    "\n",
    "      self.LR = 1e-4\n",
    "      self.TAU = 0.005\n",
    "\n",
    "      self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "    else:\n",
    "      self.policy_net.eval()\n",
    "      self.target_net.eval()\n",
    "\n",
    "      if weight_path is not None:\n",
    "        self.load_weights(weight_path)  # Load weights if path is provided\n",
    "\n",
    "  def load_weights(self, path):\n",
    "    state_dict = torch.load(path)\n",
    "    self.policy_net.load_state_dict(state_dict)\n",
    "    self.target_net.load_state_dict(state_dict)\n",
    "    print(\"Weights loaded successfully from\", path)\n",
    "  \n",
    "  def save_weights(self, path):\n",
    "    torch.save(self.policy_net.state_dict(), path)\n",
    "    print(\"Weights saved successfully to\", path)\n",
    "  \n",
    "  def create_indicator_array(self, coords, num_rows=4, num_cols=4):\n",
    "    indicator_array = torch.zeros(16)\n",
    "    \n",
    "    for row, col in coords:\n",
    "      index = row * num_cols + col\n",
    "      indicator_array[index] = 1\n",
    "    \n",
    "    return indicator_array\n",
    "  \n",
    "  def creat_index_array(self, coords, num_rows=4, num_cols=4):\n",
    "    index_array = []\n",
    "    \n",
    "    for row, col in coords:\n",
    "      index = row * num_cols + col\n",
    "      index_array.append(index)\n",
    "    \n",
    "    return torch.tensor(index_array, device=device)\n",
    "\n",
    "  def findBestMove(self, board, possible_move, player):\n",
    "    if len(possible_move) == 0:\n",
    "      return None\n",
    "    \n",
    "    if self.training:\n",
    "      sample = random.random()\n",
    "      eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "      self.steps_done += 1\n",
    "\n",
    "      if sample > eps_threshold:\n",
    "        self.policy_net.eval()\n",
    "        self.target_net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "          # Change the board to the player's perspective\n",
    "          current_board = board * player\n",
    "          state = torch.tensor(current_board, dtype=torch.float32).to(device)\n",
    "          score = self.policy_net(state) * self.create_indicator_array(possible_move)\n",
    "          return score.max(1).indices.view(1, 1)\n",
    "      else:\n",
    "        return torch.tensor([[random.choice(self.creat_index_array(possible_move))]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "        # Change the board to the player's perspective\n",
    "        current_board = board * player\n",
    "        state = torch.tensor(current_board, dtype=torch.float32).to(device)\n",
    "        score = self.policy_net(state) * self.create_indicator_array(possible_move)\n",
    "        return score.max(1).indices.view(1, 1)\n",
    "  \n",
    "  def optimize_model(self):\n",
    "    self.policy_net.train()\n",
    "    self.target_net.train()\n",
    "\n",
    "    if len(self.memory) < self.BATCH_SIZE:\n",
    "      return\n",
    "    transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "    # for param in self.policy_net.parameters():\n",
    "    #   param.grad.data.clamp_(-1, 1)\n",
    "    self.optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
