{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (0.29.1)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (1.24.4)\n",
      "Requirement already satisfied: matplotlib in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: pygame in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (2.5.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from gymnasium) (4.10.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from gymnasium) (7.1.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from gymnasium[box2d]) (2.3.5)\n",
      "Requirement already satisfied: swig==4.* in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from gymnasium[box2d]) (4.2.1)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from torch) (3.13.3)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (4.50.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from matplotlib) (6.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.18.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium 'gymnasium[box2d]' torch numpy matplotlib pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # discount factor\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        if random.random() > self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                action_values = self.model(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.randrange(self.action_size)\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            print(state.shape)\n",
    "            state = torch.FloatTensor(state)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            action = torch.LongTensor([action])\n",
    "            reward = torch.FloatTensor([reward])\n",
    "            done = torch.FloatTensor([float(done)])\n",
    "\n",
    "            Q_values = self.model(state).gather(1, action.view(-1, 1))\n",
    "            Q_values_next = self.model(next_state).detach().max(1)[0].view(-1, 1)\n",
    "            target = reward.view(-1, 1) + (self.gamma * Q_values_next * (1 - done.view(-1, 1)))\n",
    "            loss = F.mse_loss(Q_values, target)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.7453625e-03  1.3427243e-05 -1.7508660e-03 -1.6000090e-02\n",
      "  9.2581645e-02  4.0645353e-03  8.5968912e-01 -1.8291874e-03\n",
      "  1.0000000e+00  3.2873984e-02  4.0643653e-03  8.5348517e-01\n",
      " -2.7655056e-03  1.0000000e+00  4.4081330e-01  4.4581941e-01\n",
      "  4.6142203e-01  4.8954940e-01  5.3410190e-01  6.0246003e-01\n",
      "  7.0914775e-01  8.8593036e-01  1.0000000e+00  1.0000000e+00]\n",
      "[-0.02189586 -0.05299488 -0.0159262   0.0215581  -0.25590682 -0.3692084\n",
      "  1.4619732   0.9931907   1.          0.32591367  0.48922473  0.18932688\n",
      "  0.59803146  1.          0.45349255  0.45864263  0.47469407  0.50363046\n",
      "  0.54946446  0.6197888   0.7295452   0.91141266  1.          1.        ]\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "init_state = env.reset()\n",
    "print(init_state[0])\n",
    "action= env.action_space.sample()\n",
    "next_state, reward, done, truncated, _ =env.step(action)\n",
    "print(next_state)\n",
    "\n",
    "# state_size = env.observation_space.shape[0]\n",
    "# print(env.action_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/cm/lb23lq6s6f319w__2lgw1zfm0000gn/T/ipykernel_11402/2704620012.py:32: DeprecationWarning: an integer is required (got type numpy.float32).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  action = torch.LongTensor([action])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Size does not match at dimension 0 expected index [4, 1] to be smaller than self [1, 4] apart from dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(agent\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 36\u001b[0m, in \u001b[0;36mAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     33\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([reward])\n\u001b[1;32m     34\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([\u001b[38;5;28mfloat\u001b[39m(done)])\n\u001b[0;32m---> 36\u001b[0m Q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m Q_values_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(next_state)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     38\u001b[0m target \u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m Q_values_next \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Size does not match at dimension 0 expected index [4, 1] to be smaller than self [1, 4] apart from dimension 1"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.shape[0]\n",
    "agent = Agent(state_size, action_size)\n",
    "episodes = 10\n",
    "batch_size = 32\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    # print(len(state))\n",
    "    # print(state[0])\n",
    "    state = np.reshape(state[0], [1, state_size])\n",
    "    # print(state)\n",
    "\n",
    "    for time in range(500):  # maximum time per episode\n",
    "        # action = agent.act(state)\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        env.render()\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {e}/{episodes}, Score: {time}, Epsilon: {agent.epsilon:.2}\")\n",
    "            break\n",
    "\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay(batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
