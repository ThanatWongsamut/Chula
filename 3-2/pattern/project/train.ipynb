{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.8.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "importing Jupyter notebook from environment.ipynb\n",
      "importing Jupyter notebook from qAgent.ipynb\n",
      "importing Jupyter notebook from qAgentConv.ipynb\n",
      "importing Jupyter notebook from qAgentConv3D.ipynb\n",
      "importing Jupyter notebook from dumbAgent.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pygame\n",
    "from environment import TicTacToe3D\n",
    "from qAgent import QAgent\n",
    "from qAgentConv import QAgentConv\n",
    "from qAgentConv3D import QAgentConv3D\n",
    "from dumbAgent import DumbAgent\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_scores1 = []\n",
    "episode_scores2 = []\n",
    "moves = []\n",
    "\n",
    "def plot_moves(show_result=False):\n",
    "    plt.figure(1)\n",
    "    moves_t = torch.tensor(moves, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moves')\n",
    "    \n",
    "    plt.plot(moves, color='red', alpha=0.2)\n",
    "\n",
    "    if len(moves_t) >= 100:\n",
    "        means = moves_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    \n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "def plot_scores(show_result=False):\n",
    "    plt.figure(1)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Final Score')\n",
    "\n",
    "    plt.plot(episode_scores1, label='My bot play first')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(1)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Final Score')\n",
    "\n",
    "    plt.plot(episode_scores2, label='My bot play second')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    # plt.legend(loc=\"upper left\")\n",
    "\n",
    "    # plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    # if is_ipython:\n",
    "    #     if not show_result:\n",
    "    #         display.display(plt.gcf())\n",
    "    #         display.clear_output(wait=True)\n",
    "    #     else:\n",
    "    #         display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe3D(headless=False)\n",
    "# qAgent = QAgent(training=True)\n",
    "qAgent = QAgentConv3D(training=True)\n",
    "# qAgent.load_weights(\"./weights/qlearn.pth\")\n",
    "dumpAgent = DumbAgent(depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalScore(t, result, player, is_first_player=True):\n",
    "    if result == 0:\n",
    "        return 0\n",
    "    elif result == player:\n",
    "        if is_first_player:\n",
    "            return (71 - t) / 64\n",
    "        else:\n",
    "            return (72 - t) / 64\n",
    "    else:\n",
    "        if is_first_player:\n",
    "            return ((t - 8) / 56) - 1\n",
    "        else:\n",
    "            return ((t - 7) / 56) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m     col \u001b[38;5;241m=\u001b[39m act \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(f\"{player} {row+1}, {col+1}\")\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# terminated = lose\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     movable, observation, possible_move, reward, terminated \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     env\u001b[38;5;241m.\u001b[39mdraw_figures()\n\u001b[1;32m     51\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m<string>:136\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(self, row, col, player)\u001b[0m\n",
      "File \u001b[0;32m<string>:115\u001b[0m, in \u001b[0;36mevalMove\u001b[0;34m(self, row, col, player)\u001b[0m\n",
      "File \u001b[0;32m<string>:80\u001b[0m, in \u001b[0;36mcreateLosingChance\u001b[0;34m(future_board1, player)\u001b[0m\n",
      "File \u001b[0;32m<string>:160\u001b[0m, in \u001b[0;36mcheck\u001b[0;34m(self, board)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, possible_move = env.reset()\n",
    "    env.draw_lines()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).view(-1, 64)\n",
    "\n",
    "    # print(state)\n",
    "\n",
    "    first_player = 1 if i_episode % 2 == 0 else -1\n",
    "    player = first_player\n",
    "\n",
    "    if i_episode % 1000 == 0:\n",
    "        qAgent.save_weights(f\"./weights/qlearn-conv3d-{i_episode}.pth\")\n",
    "\n",
    "    t = 0\n",
    "    while True:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "\n",
    "        if player == 1:\n",
    "            action = qAgent.findBestMove(board=state * player, possible_move=possible_move, player=player)\n",
    "        else:\n",
    "            action = qAgent.findBestMove(board=state * player, possible_move=possible_move, player=player)\n",
    "\n",
    "            # action = dumpAgent.findBestMove(board=state.view(4,4,4).cpu().detach().numpy(), possible_move=possible_move, player=player)\n",
    "            # if action != None:\n",
    "            #     action = torch.tensor([4 * action[0] + action[1]], device=device).unsqueeze(0)\n",
    "\n",
    "        # print(f\"{player} {action}\")\n",
    "\n",
    "        if action == None:\n",
    "            # terminated = draw\n",
    "            movable = False\n",
    "            reward = 0\n",
    "            terminated = True\n",
    "        else:\n",
    "            act = action.item()\n",
    "\n",
    "            # print(act)\n",
    "            row = act // 4\n",
    "            col = act % 4\n",
    "            # print(f\"{player} {row+1}, {col+1}\")\n",
    "            # terminated = lose\n",
    "            movable, observation, possible_move, reward, terminated = env.move(row, col, player)\n",
    "\n",
    "            env.draw_figures()\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or (action == None)\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0).view(-1, 64)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        # action_remember = torch.zeros(16, dtype=torch.int64, device=device)\n",
    "        # action_remember[action.item()] = 1\n",
    "        if action != None:\n",
    "            if next_state == None:\n",
    "                qAgent.memory.push(state * player, action, None, reward)\n",
    "            else:\n",
    "                qAgent.memory.push(state * player, action, next_state * player, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "        if movable:\n",
    "            player = -player\n",
    "            t += 1\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        qAgent.optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = qAgent.target_net.state_dict()\n",
    "        policy_net_state_dict = qAgent.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*qAgent.TAU + target_net_state_dict[key]*(1-qAgent.TAU)\n",
    "        qAgent.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            if first_player == 1:\n",
    "                episode_scores1.append(finalScore(t, env.check(), 1, is_first_player=True))\n",
    "            else:\n",
    "                episode_scores2.append(finalScore(t, env.check(), 1, is_first_player=False))\n",
    "            moves.append(t)\n",
    "            # episode_durations.append(finalScore(t, reward.item(), player))\n",
    "            plot_scores()\n",
    "            plt.show()\n",
    "            plot_moves()\n",
    "            break\n",
    "\n",
    "qAgent.save_weights(\"./weights/qlearn-conv3d.pth\")\n",
    "\n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "plot_moves(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qAgent.save_weights(\"./weights/qlearn.pth\")\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
