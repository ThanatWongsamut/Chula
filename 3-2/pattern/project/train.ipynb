{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.8.19)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "importing Jupyter notebook from environment.ipynb\n",
      "importing Jupyter notebook from qAgent.ipynb\n",
      "importing Jupyter notebook from dumbAgent.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pygame\n",
    "from environment import TicTacToe3D\n",
    "from qAgent import QAgent\n",
    "from dumbAgent import DumbAgent\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "  from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_scores1 = []\n",
    "episode_scores2 = []\n",
    "moves = []\n",
    "\n",
    "def plot_moves(show_result=False):\n",
    "    plt.figure(1)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moves')\n",
    "    \n",
    "    plt.plot(moves, color='red', alpha=0.2)\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "def plot_scores(show_result=False):\n",
    "    plt.figure(1)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Final Score')\n",
    "\n",
    "    plt.plot(episode_scores1, label='Player 1')\n",
    "\n",
    "    # plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    # if is_ipython:\n",
    "    #     if not show_result:\n",
    "    #         display.display(plt.gcf())\n",
    "    #         display.clear_output(wait=True)\n",
    "    #     else:\n",
    "    #         display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TicTacToe3D(headless=False)\n",
    "qAgent = QAgent(training=True)\n",
    "dumpAgent = DumbAgent(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finalScore(t, result, player):\n",
    "    if result == 0:\n",
    "        return 0\n",
    "    elif result == player:\n",
    "        return (71.5 - 0.5 * player - t) / 64\n",
    "    else:\n",
    "        return ((t - 7.5 - 0.5 * player) / 56) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m qAgent\u001b[38;5;241m.\u001b[39mfindBestMove(board\u001b[38;5;241m=\u001b[39mstate, possible_move\u001b[38;5;241m=\u001b[39mpossible_move, player\u001b[38;5;241m=\u001b[39mplayer)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mdumpAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindBestMove\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpossible_move\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpossible_move\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m action[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m action[\u001b[38;5;241m1\u001b[39m]], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m<string>:92\u001b[0m, in \u001b[0;36mfindBestMove\u001b[0;34m(self, board, possible_move, player)\u001b[0m\n",
      "File \u001b[0;32m<string>:47\u001b[0m, in \u001b[0;36mminimax\u001b[0;34m(self, board, player, depth, alpha, beta, minimizingPlayer)\u001b[0m\n",
      "File \u001b[0;32m<string>:38\u001b[0m, in \u001b[0;36mminimax\u001b[0;34m(self, board, player, depth, alpha, beta, minimizingPlayer)\u001b[0m\n",
      "File \u001b[0;32m<string>:10\u001b[0m, in \u001b[0;36mcount_wins\u001b[0;34m(self, board, player)\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pattern-project/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get its state\n",
    "    state, possible_move = env.reset()\n",
    "    env.draw_lines()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0).view(-1, 64)\n",
    "\n",
    "    # print(state)\n",
    "\n",
    "    player = 1\n",
    "\n",
    "    state = state * player\n",
    "\n",
    "    for t in count():\n",
    "        if player == 1:\n",
    "            action = qAgent.findBestMove(board=state, possible_move=possible_move, player=player)\n",
    "        else:\n",
    "            action = dumpAgent.findBestMove(board=state.detach().numpy().reshape(4,4,4), possible_move=possible_move, player=player)\n",
    "            if action != None:\n",
    "                action = torch.tensor([4 * action[0] + action[1]], device=device).unsqueeze(0)\n",
    "\n",
    "        # print(f\"{player} {action}\")\n",
    "\n",
    "        if action == None:\n",
    "            # terminated = draw\n",
    "            reward = 0\n",
    "            terminated = True\n",
    "        else:\n",
    "            act = action.item()\n",
    "\n",
    "            # print(act)\n",
    "            row = act // 4\n",
    "            col = act % 4\n",
    "            # print(f\"{player} {row+1}, {col+1}\")\n",
    "            # terminated = lose\n",
    "            movable, observation, possible_move, reward, terminated = env.move(row, col, player)\n",
    "\n",
    "            env.draw_figures()\n",
    "\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or (action == None)\n",
    "\n",
    "        player = -player\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation * player, dtype=torch.float32, device=device).unsqueeze(0).view(-1, 64)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        # action_remember = torch.zeros(16, dtype=torch.int64, device=device)\n",
    "        # action_remember[action.item()] = 1\n",
    "        qAgent.memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        qAgent.optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = qAgent.target_net.state_dict()\n",
    "        policy_net_state_dict = qAgent.policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*qAgent.TAU + target_net_state_dict[key]*(1-qAgent.TAU)\n",
    "        qAgent.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_scores1.append(finalScore(t+1, env.check(), 1))\n",
    "            episode_scores2.append(finalScore(t+1, env.check(), -1))\n",
    "            moves.append(t+1)\n",
    "            # episode_durations.append(finalScore(t, reward.item(), player))\n",
    "            plot_scores()\n",
    "            plt.show()\n",
    "            plot_moves()\n",
    "            break\n",
    "\n",
    "qAgent.save_weights(\"./weights/qlearn.pth\")\n",
    "\n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "plot_moves(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
