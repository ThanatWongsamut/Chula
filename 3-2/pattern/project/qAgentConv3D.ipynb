{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "  def __init__(self, capacity):\n",
    "    self.capacity = capacity\n",
    "    self.memory = deque(maxlen=capacity)\n",
    "  \n",
    "  def push(self, *args):\n",
    "    self.memory.append(Transition(*args))\n",
    "  \n",
    "  def sample(self, batch_size):\n",
    "    return random.sample(self.memory, batch_size)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNConv3D(nn.Module):\n",
    "  def __init__(self, n_observations, n_actions, n_hidden=256):\n",
    "    super(DQNConv3D, self).__init__()\n",
    "    self.n_observations = n_observations\n",
    "    self.n_actions = n_actions\n",
    "\n",
    "    self.conv1 = nn.Conv3d(1, 16, kernel_size=2, stride=1, padding=1)\n",
    "    self.conv2 = nn.Conv3d(16, 32, kernel_size=2, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(32 * 4 * 4 * 4, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.output_layer = nn.Linear(256, n_actions)\n",
    "\n",
    "    # output layer 128 => 16\n",
    "    # self.output_layer = nn.Linear(128, n_actions)\n",
    "\n",
    "    nn.init.kaiming_normal_(self.conv1.weight, mode='fan_in', nonlinearity='relu')\n",
    "    nn.init.kaiming_normal_(self.conv2.weight, mode='fan_in', nonlinearity='relu')\n",
    "    # nn.init.kaiming_normal_(self.conv3.weight, mode='fan_in', nonlinearity='relu')\n",
    "    nn.init.kaiming_normal_(self.fc1.weight, mode='fan_in', nonlinearity='relu')\n",
    "    nn.init.kaiming_normal_(self.fc2.weight, mode='fan_in', nonlinearity='relu')\n",
    "    # nn.init.kaiming_normal_(self.fc3.weight, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 4, 4, 4).unsqueeze(1)\n",
    "    \n",
    "    x = torch.relu(self.conv1(x))\n",
    "    x = torch.relu(self.conv2(x))\n",
    "    x = x.view(-1, 32 * 4 * 4 * 4)\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = torch.relu(self.fc2(x))\n",
    "    x = self.output_layer(x)\n",
    "\n",
    "    # x = F.relu(self.fc1(x))\n",
    "    # x = F.relu(self.fc2(x))\n",
    "    # x = F.relu(self.fc3(x))\n",
    "    # x = self.output_layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgentConv3D:\n",
    "  def __init__(self, logging=False, training=False):\n",
    "    self.training = training\n",
    "\n",
    "    self.n_observations = 4*4*4\n",
    "    self.n_actions = 16\n",
    "\n",
    "    self.policy_net = DQNConv3D(self.n_observations, self.n_actions).to(device)\n",
    "    self.target_net = DQNConv3D(self.n_observations, self.n_actions).to(device)\n",
    "    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    if training:\n",
    "      self.BATCH_SIZE = 256\n",
    "      self.memory = ReplayMemory(100000)\n",
    "      self.steps_done = 0\n",
    "\n",
    "      self.GAMMA = 0.995\n",
    "      self.EPS_START = 0.9\n",
    "      self.EPS_END = 0.05\n",
    "      self.EPS_DECAY = 6000\n",
    "\n",
    "      self.LR = 1e-3\n",
    "      self.TAU = 0.005\n",
    "\n",
    "      self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "      self.loss = nn.MSELoss()\n",
    "    else:\n",
    "      self.policy_net.eval()\n",
    "      self.target_net.eval()\n",
    "\n",
    "  def load_weights(self, path):\n",
    "    state_dict = torch.load(path)\n",
    "    self.policy_net.load_state_dict(state_dict)\n",
    "    self.target_net.load_state_dict(state_dict)\n",
    "    print(\"Weights loaded successfully from\", path)\n",
    "  \n",
    "  def save_weights(self, path):\n",
    "    torch.save(self.policy_net.state_dict(), path)\n",
    "    print(\"Weights saved successfully to\", path)\n",
    "  \n",
    "  def create_indicator_array(self, coords, num_rows=4, num_cols=4):\n",
    "    indicator_array = torch.zeros(16, device=device)\n",
    "    \n",
    "    for row, col in coords:\n",
    "      index = row * num_cols + col\n",
    "      indicator_array[index] = 1\n",
    "    \n",
    "    return indicator_array\n",
    "  \n",
    "  def creat_index_array(self, coords, num_rows=4, num_cols=4):\n",
    "    index_array = []\n",
    "    \n",
    "    for row, col in coords:\n",
    "      index = row * num_cols + col\n",
    "      index_array.append(index)\n",
    "    \n",
    "    return torch.tensor(index_array, device=device)\n",
    "\n",
    "  def findBestMove(self, board, possible_move, player):\n",
    "    if len(possible_move) == 0:\n",
    "      return None\n",
    "    \n",
    "    if self.training:\n",
    "      sample = random.random()\n",
    "      eps_threshold = self.EPS_END + (self.EPS_START - self.EPS_END) * math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "      self.steps_done += 1\n",
    "\n",
    "      if sample > eps_threshold:\n",
    "        self.policy_net.eval()\n",
    "        self.target_net.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "          # Change the board to the player's perspective\n",
    "          state = torch.tensor(board, dtype=torch.float32).to(device)\n",
    "          mask = self.create_indicator_array(possible_move)\n",
    "          score = self.policy_net(state) * mask - 9999 * (1-mask)\n",
    "          return score.max(1).indices.view(1, 1)\n",
    "      else:\n",
    "        return torch.tensor([[random.choice(self.creat_index_array(possible_move))]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "      with torch.no_grad():\n",
    "        # Change the board to the player's perspective\n",
    "        state = torch.tensor(board, dtype=torch.float32).to(device)\n",
    "        mask = self.create_indicator_array(possible_move)\n",
    "        score = self.policy_net(state) * mask - 9999 * (1-mask)\n",
    "        return score.max(1).indices.view(1, 1)\n",
    "  \n",
    "  def optimize_model(self):\n",
    "    self.policy_net.train()\n",
    "    self.target_net.train()\n",
    "\n",
    "    if len(self.memory) < self.BATCH_SIZE:\n",
    "      return\n",
    "    transitions = self.memory.sample(self.BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "      next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
    "    expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "    loss = self.loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "    # for param in self.policy_net.parameters():\n",
    "    #   param.grad.data.clamp_(-1, 1)\n",
    "    self.optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pattern-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
