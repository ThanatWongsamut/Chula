{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8ae122e2f12b4cc8af05bc44a140be62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4dfd5a1a2c9b421cb26e342fa58d7cbc",
              "IPY_MODEL_19b1b5e602f04d40a7a8fb85391e5e91",
              "IPY_MODEL_11fca73df635421c8b7bb0190059a0fa"
            ],
            "layout": "IPY_MODEL_667055f22de24ecaabb70a1df3e52193"
          }
        },
        "4dfd5a1a2c9b421cb26e342fa58d7cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_275a72c39c834ba3b3d6b9c71f22a782",
            "placeholder": "​",
            "style": "IPY_MODEL_6b3cd693fdce4730b44c735200acb72e",
            "value": "100%"
          }
        },
        "19b1b5e602f04d40a7a8fb85391e5e91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_234a611196da4cd1b79acc2d019eb93a",
            "max": 21444401,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b47e6ddb9f54971b4f309980a1b5fd3",
            "value": 21444401
          }
        },
        "11fca73df635421c8b7bb0190059a0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0e444793374473ab8f6ed2f217ed961",
            "placeholder": "​",
            "style": "IPY_MODEL_531fb2e04df24d96966d2ec955993c60",
            "value": " 20.5M/20.5M [00:00&lt;00:00, 54.5MB/s]"
          }
        },
        "667055f22de24ecaabb70a1df3e52193": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "275a72c39c834ba3b3d6b9c71f22a782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b3cd693fdce4730b44c735200acb72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "234a611196da4cd1b79acc2d019eb93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b47e6ddb9f54971b4f309980a1b5fd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0e444793374473ab8f6ed2f217ed961": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "531fb2e04df24d96966d2ec955993c60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activity 9 : Machine Learning - Part 3 : Image Classification"
      ],
      "metadata": {
        "id": "VHEqg6H916rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, we will build a machine learning model that's capable of differentiating between different flavors UHT drinkable yoguht, UHT milk, or sparkling water (depending on your room)"
      ],
      "metadata": {
        "id": "lC6E_pw92BqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 0 : Make a copy of this notebook\n",
        "\n",
        "Before we begin, make a copy of this notebook first."
      ],
      "metadata": {
        "id": "6lMmUjiI2wIm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 : Config Colab for ML usage\n",
        "\n",
        "Config colab for ML usage, as done in the tutorial\n",
        "\n",
        "check if configuration was done correctly by running the cells below"
      ],
      "metadata": {
        "id": "ZM264mUX23sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QqVGy5l027P_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e55cfcc-c7bc-49d9-e0a9-daa72942e0fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 29 04:19:06 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yhdP-NYNAgI3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 : Mount Google Drive\n",
        "\n",
        "Mount your Google Drive with this colab VM to use images you've uploaded prior"
      ],
      "metadata": {
        "id": "an0bcdxj3JsK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "G9H2RVEl3VY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2d52091-c30c-4e29-b580-a5f3dcc55e1c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 : Prepare data for model training\n",
        "\n",
        "Run the code below to prepare your data for training\n",
        "\n",
        "**Do not modify the code!**"
      ],
      "metadata": {
        "id": "n8RWYT103YGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path = f'/content/drive/My Drive/CEE_Act9-3/train'"
      ],
      "metadata": {
        "id": "qmD6BEJA3jyc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "dataset_main = ImageFolder(train_img_path, transform=transform)\n",
        "dataset_tmp = torch.utils.data.random_split(dataset_main, [0.9, 0.1])\n",
        "\n",
        "dataset_train = dataset_tmp[0]\n",
        "dataset_val = dataset_tmp[1]"
      ],
      "metadata": {
        "id": "oWCi7QLZ39JW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 16             # If you get GPU memory error, reduce this value by half at a time\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(dataset_val, batch_size=batch_size, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "LiIFVaG_4eJa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 : Train the model\n",
        "\n",
        "Run the code below to train your machine learning model, this will take a few minutes\n",
        "\n",
        "**Do not modify the code!**"
      ],
      "metadata": {
        "id": "crTl4VS55I6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "\n",
        "class ImageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageModel, self).__init__()\n",
        "\n",
        "        self.efficientnet = efficientnet_b0(weights=EfficientNet_B0_Weights.DEFAULT)\n",
        "        self.preprocess = EfficientNet_B0_Weights.DEFAULT.transforms()\n",
        "        for param in self.efficientnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.efficientnet.classifier[1] = nn.Linear(1280, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.preprocess(x)\n",
        "        x = self.efficientnet(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Cc7EQjfk4W7R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7XJ2XmrgCHdj"
      },
      "outputs": [],
      "source": [
        "import copy, time\n",
        "\n",
        "def train_model(model, dataloader_train, dataloader_val, num_epoch, lossfn, optimizer, save_path=None):\n",
        "    best_model_weight = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "    val_acc_his = []\n",
        "\n",
        "    for epoch in range(num_epoch):\n",
        "        print(f'epoch {epoch+1} / {num_epoch}')\n",
        "        start_time = time.time()\n",
        "        model.train()\n",
        "\n",
        "        running_loss_train = 0.0\n",
        "        running_loss_val = 0.0\n",
        "\n",
        "        for image, label in dataloader_train:\n",
        "            image = image.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(image)\n",
        "            loss = lossfn(output, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss_train += loss.item() * image.size(0)\n",
        "\n",
        "        epoch_loss_train = running_loss_train / (len(dataloader_train.dataset) * 5)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for image, label in dataloader_val:\n",
        "            image = image.to(device)\n",
        "            label = label.to(device)\n",
        "\n",
        "            output = model(image)\n",
        "            loss = lossfn(output, label)\n",
        "\n",
        "            running_loss_val += loss.item() * image.size(0)\n",
        "        \n",
        "        epoch_loss_val = running_loss_val / len(dataloader_val.dataset)\n",
        "        val_acc_his.append(epoch_loss_val)\n",
        "\n",
        "        print(f'epoch {epoch+1} training completed ; training loss: {epoch_loss_train:.6f} ; validation loss: {epoch_loss_val:.6f}')\n",
        "\n",
        "        if epoch_loss_val < best_loss:\n",
        "            print('validation loss improved, saving model')\n",
        "            best_loss = epoch_loss_val\n",
        "            best_model_weight = copy.deepcopy(model.state_dict())\n",
        "            if save_path: torch.save(model.state_dict(), save_path)\n",
        "        else:\n",
        "            print(f'validation loss did not improved from {best_loss:.6f}')\n",
        "\n",
        "        print(f'epoch {epoch+1} completed in {time.time() - start_time:.4f} seconds')\n",
        "        print()\n",
        "\n",
        "    model.load_state_dict(best_model_weight)\n",
        "    return model, val_acc_his"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pvRPZmtxaoQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "8ae122e2f12b4cc8af05bc44a140be62",
            "4dfd5a1a2c9b421cb26e342fa58d7cbc",
            "19b1b5e602f04d40a7a8fb85391e5e91",
            "11fca73df635421c8b7bb0190059a0fa",
            "667055f22de24ecaabb70a1df3e52193",
            "275a72c39c834ba3b3d6b9c71f22a782",
            "6b3cd693fdce4730b44c735200acb72e",
            "234a611196da4cd1b79acc2d019eb93a",
            "9b47e6ddb9f54971b4f309980a1b5fd3",
            "f0e444793374473ab8f6ed2f217ed961",
            "531fb2e04df24d96966d2ec955993c60"
          ]
        },
        "outputId": "a8a338df-f295-47e0-a427-415dc0aa12c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/20.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ae122e2f12b4cc8af05bc44a140be62"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model = ImageModel()\n",
        "model.to(device)\n",
        "\n",
        "lossfn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = f'/content/drive/My Drive/CEE_Act9-3/model.pt'"
      ],
      "metadata": {
        "id": "POYim93M7-PW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_cnt = 20\n",
        "model, val_acc_his = train_model(model, train_loader, val_loader, epoch_cnt, lossfn, optimizer, model_save_path)"
      ],
      "metadata": {
        "id": "Wu5VJShs8S3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b11a01d3-2400-4327-9c60-7c23113ac8ec"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1 / 20\n",
            "epoch 1 training completed ; training loss: 0.192900 ; validation loss: 0.485148\n",
            "validation loss improved, saving model\n",
            "epoch 1 completed in 194.8654 seconds\n",
            "\n",
            "epoch 2 / 20\n",
            "epoch 2 training completed ; training loss: 0.080001 ; validation loss: 0.222799\n",
            "validation loss improved, saving model\n",
            "epoch 2 completed in 35.7371 seconds\n",
            "\n",
            "epoch 3 / 20\n",
            "epoch 3 training completed ; training loss: 0.053343 ; validation loss: 0.153114\n",
            "validation loss improved, saving model\n",
            "epoch 3 completed in 33.8148 seconds\n",
            "\n",
            "epoch 4 / 20\n",
            "epoch 4 training completed ; training loss: 0.037158 ; validation loss: 0.100605\n",
            "validation loss improved, saving model\n",
            "epoch 4 completed in 32.0805 seconds\n",
            "\n",
            "epoch 5 / 20\n",
            "epoch 5 training completed ; training loss: 0.031327 ; validation loss: 0.080891\n",
            "validation loss improved, saving model\n",
            "epoch 5 completed in 33.3278 seconds\n",
            "\n",
            "epoch 6 / 20\n",
            "epoch 6 training completed ; training loss: 0.022458 ; validation loss: 0.061251\n",
            "validation loss improved, saving model\n",
            "epoch 6 completed in 32.7676 seconds\n",
            "\n",
            "epoch 7 / 20\n",
            "epoch 7 training completed ; training loss: 0.018732 ; validation loss: 0.050332\n",
            "validation loss improved, saving model\n",
            "epoch 7 completed in 33.5927 seconds\n",
            "\n",
            "epoch 8 / 20\n",
            "epoch 8 training completed ; training loss: 0.015531 ; validation loss: 0.045106\n",
            "validation loss improved, saving model\n",
            "epoch 8 completed in 32.2284 seconds\n",
            "\n",
            "epoch 9 / 20\n",
            "epoch 9 training completed ; training loss: 0.015023 ; validation loss: 0.039245\n",
            "validation loss improved, saving model\n",
            "epoch 9 completed in 34.0046 seconds\n",
            "\n",
            "epoch 10 / 20\n",
            "epoch 10 training completed ; training loss: 0.012459 ; validation loss: 0.029986\n",
            "validation loss improved, saving model\n",
            "epoch 10 completed in 32.3286 seconds\n",
            "\n",
            "epoch 11 / 20\n",
            "epoch 11 training completed ; training loss: 0.013878 ; validation loss: 0.038268\n",
            "validation loss did not improved from 0.029986\n",
            "epoch 11 completed in 34.1076 seconds\n",
            "\n",
            "epoch 12 / 20\n",
            "epoch 12 training completed ; training loss: 0.014607 ; validation loss: 0.026009\n",
            "validation loss improved, saving model\n",
            "epoch 12 completed in 32.9027 seconds\n",
            "\n",
            "epoch 13 / 20\n",
            "epoch 13 training completed ; training loss: 0.010033 ; validation loss: 0.026781\n",
            "validation loss did not improved from 0.026009\n",
            "epoch 13 completed in 32.9739 seconds\n",
            "\n",
            "epoch 14 / 20\n",
            "epoch 14 training completed ; training loss: 0.010667 ; validation loss: 0.019554\n",
            "validation loss improved, saving model\n",
            "epoch 14 completed in 33.4212 seconds\n",
            "\n",
            "epoch 15 / 20\n",
            "epoch 15 training completed ; training loss: 0.009068 ; validation loss: 0.021324\n",
            "validation loss did not improved from 0.019554\n",
            "epoch 15 completed in 30.8157 seconds\n",
            "\n",
            "epoch 16 / 20\n",
            "epoch 16 training completed ; training loss: 0.007507 ; validation loss: 0.014620\n",
            "validation loss improved, saving model\n",
            "epoch 16 completed in 33.0804 seconds\n",
            "\n",
            "epoch 17 / 20\n",
            "epoch 17 training completed ; training loss: 0.006949 ; validation loss: 0.016380\n",
            "validation loss did not improved from 0.014620\n",
            "epoch 17 completed in 31.9343 seconds\n",
            "\n",
            "epoch 18 / 20\n",
            "epoch 18 training completed ; training loss: 0.006653 ; validation loss: 0.014831\n",
            "validation loss did not improved from 0.014620\n",
            "epoch 18 completed in 32.8652 seconds\n",
            "\n",
            "epoch 19 / 20\n",
            "epoch 19 training completed ; training loss: 0.006345 ; validation loss: 0.018685\n",
            "validation loss did not improved from 0.014620\n",
            "epoch 19 completed in 31.8446 seconds\n",
            "\n",
            "epoch 20 / 20\n",
            "epoch 20 training completed ; training loss: 0.006979 ; validation loss: 0.014080\n",
            "validation loss improved, saving model\n",
            "epoch 20 completed in 32.1002 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 : Test the model\n",
        "\n",
        "Now, let's test the model with another set of data!\n",
        "\n",
        "**Do not modify the code except the one marked with comments!**"
      ],
      "metadata": {
        "id": "wUV-F2-t_xHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/leo2tigers/compengess2023-act9.git'"
      ],
      "metadata": {
        "id": "MEmSM9pe_wgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83cec44c-014c-4448-9698-6043b506b53d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'compengess2023-act9'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 48 (delta 0), reused 5 (delta 0), pack-reused 42\u001b[K\n",
            "Unpacking objects: 100% (48/48), 88.08 MiB | 13.98 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your room no. (401, 404, 502) here\n",
        "room_no = 401"
      ],
      "metadata": {
        "id": "A3sZPCaN_9Fo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.datasets.folder import default_loader\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, paths, transform=None):\n",
        "        self.paths = paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = default_loader(f'compengess2023-act9/test/{room_no}/{self.paths[index]}')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image\n",
        "\n",
        "def test_model(img_paths):\n",
        "    image = ImageDataset(img_paths, transform=transform)\n",
        "    loader = torch.utils.data.DataLoader(image, batch_size=1)\n",
        "\n",
        "    all_predictions = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for img in loader:\n",
        "            predictions = list(nn.Softmax(dim=1)(model(img.to(device))).cpu().numpy())\n",
        "            for prediction in predictions:\n",
        "                all_predictions.append(prediction)\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "def detail_result(label_list, filename_and_pred):\n",
        "    print(filename_and_pred[0])\n",
        "    for i in range(len(label_list)):\n",
        "        print(f'{label_list[i]}: {filename_and_pred[1][i]*100:.3f}%')\n",
        "    print()"
      ],
      "metadata": {
        "id": "04w1GaolAE9e"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = sorted(os.listdir(f\"/content/drive/My Drive/CEE_Act9-3/train\"))\n",
        "file_list = os.listdir(f'./compengess2023-act9/test/{room_no}')\n",
        "res = list(zip(file_list, test_model(file_list)))\n",
        "for entry in res: detail_result(label_list, entry)"
      ],
      "metadata": {
        "id": "Cdpizmqk_n1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946b0ac4-8d8e-4e52-cd1d-42078ec20104"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IMG_20230329_090203.jpg\n",
            "dutchmill_mixed: 1.042%\n",
            "dutchmill_orange: 0.887%\n",
            "dutchmill_passionfruit: 74.085%\n",
            "dutchmill_strawberry: 23.986%\n",
            "\n",
            "IMG_20230328_233244.jpg\n",
            "dutchmill_mixed: 6.846%\n",
            "dutchmill_orange: 10.930%\n",
            "dutchmill_passionfruit: 63.435%\n",
            "dutchmill_strawberry: 18.789%\n",
            "\n",
            "IMG_20230329_090921.jpg\n",
            "dutchmill_mixed: 3.231%\n",
            "dutchmill_orange: 24.668%\n",
            "dutchmill_passionfruit: 11.650%\n",
            "dutchmill_strawberry: 60.452%\n",
            "\n",
            "IMG_20230329_091020.jpg\n",
            "dutchmill_mixed: 1.474%\n",
            "dutchmill_orange: 1.606%\n",
            "dutchmill_passionfruit: 39.856%\n",
            "dutchmill_strawberry: 57.063%\n",
            "\n",
            "IMG_20230328_233318.jpg\n",
            "dutchmill_mixed: 43.902%\n",
            "dutchmill_orange: 4.947%\n",
            "dutchmill_passionfruit: 50.550%\n",
            "dutchmill_strawberry: 0.601%\n",
            "\n",
            "IMG_20230329_090258.jpg\n",
            "dutchmill_mixed: 7.178%\n",
            "dutchmill_orange: 0.192%\n",
            "dutchmill_passionfruit: 92.578%\n",
            "dutchmill_strawberry: 0.052%\n",
            "\n",
            "IMG_20230329_090321.jpg\n",
            "dutchmill_mixed: 6.956%\n",
            "dutchmill_orange: 1.071%\n",
            "dutchmill_passionfruit: 91.754%\n",
            "dutchmill_strawberry: 0.218%\n",
            "\n",
            "IMG_20230328_233301.jpg\n",
            "dutchmill_mixed: 9.011%\n",
            "dutchmill_orange: 38.169%\n",
            "dutchmill_passionfruit: 50.558%\n",
            "dutchmill_strawberry: 2.262%\n",
            "\n",
            "IMG_20230328_233338.jpg\n",
            "dutchmill_mixed: 2.449%\n",
            "dutchmill_orange: 18.601%\n",
            "dutchmill_passionfruit: 77.571%\n",
            "dutchmill_strawberry: 1.379%\n",
            "\n",
            "IMG_20230329_090940.jpg\n",
            "dutchmill_mixed: 10.094%\n",
            "dutchmill_orange: 87.074%\n",
            "dutchmill_passionfruit: 0.778%\n",
            "dutchmill_strawberry: 2.054%\n",
            "\n",
            "IMG_20230329_091004.jpg\n",
            "dutchmill_mixed: 72.332%\n",
            "dutchmill_orange: 7.749%\n",
            "dutchmill_passionfruit: 18.154%\n",
            "dutchmill_strawberry: 1.764%\n",
            "\n",
            "IMG_20230329_090231.jpg\n",
            "dutchmill_mixed: 48.461%\n",
            "dutchmill_orange: 29.456%\n",
            "dutchmill_passionfruit: 21.277%\n",
            "dutchmill_strawberry: 0.806%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Please show the result from the cell above to an instructor or TA to be graded**\n",
        "\n",
        "***- THIS IS THE END OF PART 3 -***"
      ],
      "metadata": {
        "id": "T3IDb3xLBf4q"
      }
    }
  ]
}